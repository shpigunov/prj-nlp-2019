{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Open data\n",
    "* Normalize\n",
    "* turn to vectors\n",
    "* dimensionality reduction\n",
    "* kNN or DTree classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open data\n",
    "\n",
    "import gensim\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "en_vectors_file = 'numberbatch-en.txt'\n",
    "uk_vectors_file = 'news.lowercased.tokenized.word2vec.300d'\n",
    "\n",
    "uk_vectors = KeyedVectors.load_word2vec_format(uk_vectors_file, binary=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('позашляховик', 0.8455106019973755),\n",
       " ('джип', 0.8443582057952881),\n",
       " ('мотоцикл', 0.8110474348068237),\n",
       " ('мікроавтобус', 0.7966219782829285),\n",
       " ('автомобіль**', 0.784403383731842),\n",
       " ('бус', 0.7747742533683777),\n",
       " ('легковик', 0.7658803462982178),\n",
       " ('скутер', 0.7596759796142578),\n",
       " ('фургон', 0.7572780847549438),\n",
       " ('мопед', 0.7557206749916077)]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "uk_vectors.most_similar('автомобіль', topn=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('ющенка', 0.3845677375793457),\n",
       " ('балоги', 0.37590137124061584),\n",
       " ('ющенко', 0.3565678596496582),\n",
       " ('балога', 0.3410945236682892),\n",
       " ('януковича', 0.33086124062538147),\n",
       " ('пилипишина', 0.32630905508995056),\n",
       " ('палинський', 0.3042936325073242),\n",
       " ('пинзеника', 0.30348095297813416),\n",
       " ('пилипишин', 0.3004416227340698),\n",
       " ('янукович', 0.2991790771484375)]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Try mathematically subtracting vectors\n",
    "import numpy as np\n",
    "\n",
    "def subtract(w1, w2):\n",
    "    global uk_vectors\n",
    "    return np.subtract(uk_vectors[w1], uk_vectors[w2])\n",
    "\n",
    "uk_vectors.similar_by_vector(subtract('ющенко', 'президент'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('ющенка', 0.4185105860233307), ('балога', 0.4034050703048706), ('балоги', 0.3902004361152649), ('янукович', 0.3754698634147644), ('януковича', 0.3654754161834717), ('пилипишин', 0.3459473252296448), ('пинзеник', 0.3352230191230774), ('пилипишина', 0.33186835050582886), ('палинський', 0.3314499855041504), ('гвоздь', 0.3293236494064331)]\n"
     ]
    }
   ],
   "source": [
    "# Note that the `most_similar` methor uses a similar, but not identical implementation\n",
    "print(uk_vectors.most_similar(positive=['ющенко'], negative=['президент']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m----------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                    Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-8352f61ef9f6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    143\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 145\u001b[0;31m \u001b[0mvec_map\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuild_map\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    146\u001b[0m \u001b[0mdb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdbscan\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvec_map\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    147\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-6-8352f61ef9f6>\u001b[0m in \u001b[0;36mbuild_map\u001b[0;34m(data)\u001b[0m\n\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mpost\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 102\u001b[0;31m         \u001b[0mvec_map\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvectorize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpost\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpost\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mvec_map\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-6-8352f61ef9f6>\u001b[0m in \u001b[0;36mvectorize\u001b[0;34m(post, uk_vectors)\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m     \u001b[0;31m# Tokenize and remove stopwords\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 77\u001b[0;31m     \u001b[0mwords\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mw\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtokenize_uk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize_words\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpost\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mload_stopwords\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     78\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m     \u001b[0;31m# Init empty vector\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-6-8352f61ef9f6>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m     \u001b[0;31m# Tokenize and remove stopwords\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 77\u001b[0;31m     \u001b[0mwords\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mw\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtokenize_uk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize_words\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpost\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mload_stopwords\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     78\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m     \u001b[0;31m# Init empty vector\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-6-8352f61ef9f6>\u001b[0m in \u001b[0;36mload_stopwords\u001b[0;34m()\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0mstopwords\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'stopwords.txt'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadlines\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m             \u001b[0mstopwords\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.6.5/lib/python3.6/codecs.py\u001b[0m in \u001b[0;36mdecode\u001b[0;34m(self, input, final)\u001b[0m\n\u001b[1;32m    316\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mNotImplementedError\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    317\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 318\u001b[0;31m     \u001b[0;32mdef\u001b[0m \u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfinal\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    319\u001b[0m         \u001b[0;31m# decode input (taking the buffer into account)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    320\u001b[0m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuffer\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from os import path, walk\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.cluster import DBSCAN\n",
    "from langdetect import detect\n",
    "\n",
    "from pprint import pprint as pp\n",
    "\n",
    "\n",
    "import operator\n",
    "import tokenize_uk\n",
    "import re\n",
    "\n",
    "\n",
    "def open_data(root):\n",
    "    \"\"\"Open all files in specified root folder and return a list of separated posts\"\"\"\n",
    "    \n",
    "    dataset = []\n",
    "    \n",
    "    # Walk text files in data folder\n",
    "    for root, dirs, files in walk(top=root):\n",
    "        for fname in files:\n",
    "            p = path.join(root, fname)\n",
    "            \n",
    "            # Open each file\n",
    "            with open(p, 'r') as f:\n",
    "                post_map = {}\n",
    "                content = ''\n",
    "                for line in f.readlines():\n",
    "                    content = content + line\n",
    "                \n",
    "                # Split files into individual posts\n",
    "                posts = [p for p in content.split('\\n\\n\\n') if p != '']\n",
    "                dataset.extend(posts)\n",
    "    \n",
    "    # Filter only Ukrainian posts. Also filter out posts with no features\n",
    "    try:\n",
    "        dataset = [p for p in dataset if detect(p) == 'uk']\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    return dataset\n",
    "\n",
    "def vectorize(post, uk_vectors=uk_vectors):\n",
    "    \"\"\"Obtain vector representation of a post by: \n",
    "    summing all vectors for each semantically significant word in the post;\n",
    "    and averaging by number of posts.\"\"\"\n",
    "\n",
    "    def normalize(s):\n",
    "        \"\"\"A primitive normalization routine\"\"\"\n",
    "\n",
    "        s = s.lower()\n",
    "\n",
    "        punct = set('.,-*/:;')\n",
    "\n",
    "        for c in punct:\n",
    "            s = s.replace(c, '')\n",
    "\n",
    "        return s\n",
    "    \n",
    "    def load_stopwords():\n",
    "        \"\"\"Load stopwords from an included list\"\"\"\n",
    "\n",
    "        stopwords = set()\n",
    "        with open('stopwords.txt', 'r') as f:\n",
    "            for line in f.readlines():\n",
    "                stopwords.add(line.strip())\n",
    "\n",
    "        return stopwords\n",
    "    \n",
    "    # Normalize (mostly punctuation)\n",
    "    post = normalize(post)\n",
    "        \n",
    "    # Tokenize and remove stopwords\n",
    "    words = [w for w in tokenize_uk.tokenize_words(post) if w not in load_stopwords()]\n",
    "    \n",
    "    # Init empty vector\n",
    "    vec = np.zeros(300)\n",
    "    \n",
    "    # Sum and average vectors for tokens\n",
    "    c = 0\n",
    "    for word in words:\n",
    "        try:\n",
    "            vec = np.add(vec, uk_vectors[word])\n",
    "            c += 1\n",
    "        except KeyError:\n",
    "            pass\n",
    "    \n",
    "    if c != 0:\n",
    "        vec = np.array([x / c for x in vec])\n",
    "    \n",
    "    return vec\n",
    "        \n",
    "def build_map(data):\n",
    "    \"\"\"Create a vector map of {(vector): 'post'} to look up posts by vector\"\"\"\n",
    "    \n",
    "    vec_map = {}\n",
    "    \n",
    "    for post in data:\n",
    "        vec_map[tuple(vectorize(post))] = post\n",
    "    \n",
    "    return vec_map\n",
    "\n",
    "def find_similar(s, vec_map, n=10):\n",
    "    \"\"\"Basic cosine similarity clustering algorithm\"\"\"\n",
    "    \n",
    "    vectors = [k for k in vec_map.keys()]\n",
    "    target = vectorize(s)\n",
    "    \n",
    "    sim_pairs = []\n",
    "    for i in range(0, len(vectors)):\n",
    "        a = np.array(vectors[i])\n",
    "        b = np.array(target)\n",
    "        \n",
    "        # Calculate cosine similarity\n",
    "        dot = np.dot(a, b)\n",
    "        norma = np.linalg.norm(a)\n",
    "        normb = np.linalg.norm(b)\n",
    "        cos = dot / (norma * normb)\n",
    "        \n",
    "        sim_pairs.append((vec_map[vectors[i]], cos))\n",
    "    \n",
    "    return sorted(sim_pairs, key=lambda x: x[1], reverse=True)[0:n+1]\n",
    "    \n",
    "\n",
    "def dbscan(vec_map):\n",
    "    \"\"\"Cluster posts in the dataset using the DBSCAN algorithm\"\"\"\n",
    "    \n",
    "    # Vectors act as keys in the vector map, so let's get them\n",
    "    vectors = [k for k in vec_map.keys()]\n",
    "    \n",
    "    db = DBSCAN(eps=0.5, n_jobs=-1, metric='cosine').fit(vectors)\n",
    "    labels = db.labels_\n",
    "    n_clusters = len(set(labels)) - (1 if -1 in labels else 0)\n",
    "    print(f\"Estimated {n_clusters} clusters\")\n",
    "    \n",
    "    return db\n",
    "\n",
    "    \n",
    "root = './1551'\n",
    "\n",
    "dataset = open_data(root)\n",
    "vec_map = build_map(dataset)\n",
    "# db = dbscan(vec_map)\n",
    "\n",
    "\n",
    "# s = \"\"\"нічим дихати\"\"\"\n",
    "\n",
    "# pairs = find_similar(s, vec_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('В будинку Л.Гавро 5а особливо зі сторони озера другу ніч і день неможливо '\n",
      "  'відкрити вікно, запах смогу, що це за запах, не відомо, як нам дихати? '\n",
      "  'Перевірте будьласка, чи можна дихати цим повітрям та надайте інформацію '\n",
      "  'жителям!',\n",
      "  0.4697315548502146),\n",
      " ('Прошу зробити капітальний ремон дороги в цьому дворі! Тут не те що машиною '\n",
      "  'проїхати складно, тут навіть пройти вечором страшно, ноги можна по '\n",
      "  'виламувати! Дякую',\n",
      "  0.46093911883301375),\n",
      " ('НЕ ПРИБРАНА ДВОРОВАЯ ТЕРИТОРИЯ , ГДЕ ДВОРНИКИ , НЕ ВИДНО',\n",
      "  0.43826666223715477),\n",
      " ('Додаток до мого звернення # А-8346 від 18.09.15.\\n'\n",
      "  'Скажіть, будь-ласка, на фото, що прикрепила, це дійсно норма тиску? І в цей '\n",
      "  'же час ГОРЯЧА ВОДА просто відсутня! нема нормального напору! Я зверталася з '\n",
      "  'більшою проблемою поганого тиску ГОРЯЧОЇ ВОДИ! Бо дуже неприємно, коли ти '\n",
      "  'почав приймати душ, а тут вода зникає і  стоїш намиленою у ванні і чекаєш, '\n",
      "  'коли з&#039;явиться вода.',\n",
      "  0.4289672979481472),\n",
      " ('Добрий день!\\n'\n",
      "  'Дуже цікавить коли закінчиться це знущання?\\n'\n",
      "  'Немае холодної води. Мало того, що не було дві неділі горячої води. Так ще '\n",
      "  'відключення холодної з ранку до вечора. \\n'\n",
      "  '29.01.2015 - це був четверг, а також 05,02,15 - сьогодні також нема '\n",
      "  'холодної води.\\n'\n",
      "  'По четвергам нам не очікувати холодної води? \\n'\n",
      "  'А сьогодні ви вімкнули гарячу воду! Холодну не очікувати?\\n'\n",
      "  ' Чи  це знов нема  тиску? \\n'\n",
      "  'Люди, що проживають вище 10 поверху, що їм робити?\\n'\n",
      "  'Чому не можно вирішити це питання один раз так, щоб не повертатися кожного '\n",
      "  'разу до тої ж проблеми!??',\n",
      "  0.42453362738051514),\n",
      " ('Доброго дня!\\n'\n",
      "  'В підвалі першого під’їзду прорвало каналізаційну трубу. Наскільки там все '\n",
      "  'відремонтували - не знаю, але звідти йде страшенний сморід. Якщо загальні '\n",
      "  'двері зачинені, то він відчувається навіть на 6-му поверсі! Прошу вжити '\n",
      "  'заходів щодо повної ліквідації наслідкув аварії, адже під’їзд і так '\n",
      "  'страшний - ніякого ремонту, розбиті вікна, - так тепер ще й дихати '\n",
      "  'неможливо.',\n",
      "  0.42414770158786236),\n",
      " ('Прорвало трубу в четвертому підїзді на пятому поверсі.З боку ЖЕКу КП '\n",
      "  'Шулявка нічого не робиться для усунення.Якщо вчора вода капала.тосьогодні '\n",
      "  'при відкритті дверей підїзду вона просто виливається на ноги.Вода тече '\n",
      "  'потоком з пятого аоверху по всім поверхам.Вимагаємо не тільки полагодження '\n",
      "  'труб.а й заново отштукатурити і побілити підїзд.шпаклівка уже '\n",
      "  'осипається.навіть якщо все висохне-може утворитися грибок!Дякую!',\n",
      "  0.42042137893645815),\n",
      " ('Доброго дня! На Троєщині продовжується відчуватись запах їдкого диму вночі! '\n",
      "  'І постійно це відбувається саме вночі і до самого ранку. Раніше вже писали '\n",
      "  'з приводу цієї проблеми, отримали відповідь, що багато пожеж, літо, люди '\n",
      "  'необережні і тд... Але вже спеки немає, а ситуація зі сморідом та ж сама! '\n",
      "  'Якщо це природні пожежі ( торф і тд) то чому саме вночі?? Він вдень не '\n",
      "  'горить і не тліє? Чому вдень немає такого сморіду? Раніше такого не було ( '\n",
      "  'навіть того року ми такого не відчували так багато і кожну ніч). Ми не '\n",
      "  'можемо постійно сидіти вдома і не відкривати вікон, особливо з маленькою '\n",
      "  'дитиною! Будь-ласка, надайте офіційних пояснень стосовно задимлення по '\n",
      "  'ночах на Троєщині. Також прохання надати офіційних даних стосовно '\n",
      "  'забруднення повітря в цьому районі. Мешканці в праві знати, чим дихають. '\n",
      "  'Дякуємо.',\n",
      "  0.42011253340863747),\n",
      " ('Добрий день. Скільки все звертався по цій ділянці дорожного покриття, її '\n",
      "  'все ігнорують і ні хто нічого не робить!!! Коли її зроблять, коли там '\n",
      "  'траншея буде? Дякую',\n",
      "  0.41777720628581255),\n",
      " ('Добрий день. Я живу на 9-му поверсі і вже 4-ри дні проблема з тиском води '\n",
      "  '(холодна та горяча). Вода що є, що нема. Мало того, що горяча вода дуже '\n",
      "  'довго стікає пока стане горячою, так ще й маленька струйка, під якою '\n",
      "  'взагалі не помиешся. Я маю чекати до дванадцятої ночі поки всі інші поверхи '\n",
      "  'нарешті перемиються і то не факт....бо вода якщо появилась, через хвилину '\n",
      "  'може просто знову  зникнути...\\n'\n",
      "  'У ЖЄКу не можуть дати ні яких точних пояснень! Тільки екають, '\n",
      "  'мекають...&amp;quot;Культурні&amp;quot;... Сказали тільки, що ведутся '\n",
      "  'роботи і може сьогодні до кінця дня зроблять...А це було два дні назад!!! \\n'\n",
      "  'За що ми платимо гроші чималеньки?! \\n'\n",
      "  'Прошу посприяти рішеню даного питання. Бо неможливо це вже терпіти.',\n",
      "  0.41752975062974024),\n",
      " ('Отключена горячая вода.\\nКогда включат, не известно.', 0.41328932297246174)]\n"
     ]
    }
   ],
   "source": [
    "pp(pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
