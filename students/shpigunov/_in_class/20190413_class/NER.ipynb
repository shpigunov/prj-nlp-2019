{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Open data\n",
    "* Build baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "156 73\n"
     ]
    }
   ],
   "source": [
    "PATH = \"../ner-uk/\"\n",
    "\n",
    "# Read tokens and positions of tokens from a file\n",
    "\n",
    "def read_tokens(filename):\n",
    "    tokens = []\n",
    "    pos = 0\n",
    "    with open(filename, \"r\") as f:\n",
    "        text = f.read().split(\"\\n\")\n",
    "        for line in text:\n",
    "            if len(line) == 0:\n",
    "                pos += 1\n",
    "            else:\n",
    "                tokens.append((\"<S>\", pos, pos))\n",
    "                for token in line.split(\" \"):\n",
    "                    tokens.append((token, pos, pos + len(token)))\n",
    "                    pos += len(token) + 1\n",
    "                tokens.append((\"</S>\", pos, pos))\n",
    "    return tokens\n",
    "\n",
    "# Read annotations and positions of annotations from a file\n",
    "\n",
    "def read_annotations(filename):\n",
    "    anno = []\n",
    "    with open(filename, \"r\") as f:\n",
    "        for line in f.readlines():\n",
    "            annotations = line.split()\n",
    "            anno.append((annotations[1], int(annotations[2]), int(annotations[3])))\n",
    "    return anno\n",
    "\n",
    "# Using positions of tokens and annotations, extract a list of token labels\n",
    "\n",
    "def extract_labels(anno, tokens):\n",
    "    labels = []\n",
    "    ann_id = 0\n",
    "    for token in tokens:\n",
    "        if ann_id < len(anno):\n",
    "            label, beg, end = anno[ann_id]\n",
    "            if token[0] in [\"<S>\", \"</S>\"]:\n",
    "                labels.append(\"--\")\n",
    "            elif token[1] < beg:\n",
    "                labels.append(\"--\")\n",
    "            else:\n",
    "                if token[1] == beg:\n",
    "                    labels.append(\"B-\" + label)\n",
    "                else:\n",
    "                    labels.append(\"I-\" + label)\n",
    "                if token[2] == end:\n",
    "                    ann_id += 1\n",
    "        else:\n",
    "            labels.append(\"--\")    \n",
    "    return labels\n",
    "\n",
    "# tokens = read_tokens(PATH + \"data/A_alumni.krok.edu.ua_Prokopenko_Vidrodzhennia_velotreku(5).tok.txt\")\n",
    "# anno = read_annotations(PATH + \"data/A_alumni.krok.edu.ua_Prokopenko_Vidrodzhennia_velotreku(5).tok.ann\")\n",
    "# labels = extract_labels(anno, tokens)\n",
    "\n",
    "# for i, j in zip(tokens, labels):\n",
    "#     print(i[0], j)\n",
    "\n",
    "# Extract list of files for training and testing\n",
    "\n",
    "dev_test = {\"dev\": [], \"test\": []}\n",
    "category = \"\"\n",
    "with open(PATH + \"doc/dev-test-split.txt\", \"r\") as f:\n",
    "    for line in f.readlines():\n",
    "        line = line.strip()\n",
    "        if line in [\"DEV\", \"TEST\"]:\n",
    "            category = line.lower()\n",
    "        elif len(line) == 0:\n",
    "            continue\n",
    "        else:\n",
    "            dev_test[category].append(line)\n",
    "\n",
    "print(len(dev_test[\"dev\"]), len(dev_test[\"test\"]))\n",
    "\n",
    "# Get train and test data and labels\n",
    "\n",
    "train_tokens, test_tokens, train_labels, test_labels = [], [], [], []\n",
    "\n",
    "for filename in dev_test[\"dev\"]:\n",
    "    try:\n",
    "        tokens = read_tokens(PATH + \"data/\" + filename + \".txt\")\n",
    "        train_tokens += [token[0] for token in tokens]\n",
    "        train_labels += extract_labels(read_annotations(PATH + \"data/\" + filename + \".ann\"), tokens)\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "for filename in dev_test[\"test\"]:\n",
    "    try:\n",
    "        tokens = read_tokens(PATH + \"data/\" + filename + \".txt\")\n",
    "        test_tokens += [token[0] for token in tokens]\n",
    "        test_labels += extract_labels(read_annotations(PATH + \"data/\" + filename + \".ann\"), tokens)\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<S>', 'На', 'довірливих', 'кіровоградців', 'полюють', 'шахраї', 'та', 'фірми-посередники', ',', 'які', 'за', '1000', 'грн', '.', 'готові', '«', 'виготовити', '»', 'біометричний', 'паспорт', ',', 'який', 'коштує', '518', 'грн', '.', '</S>', '<S>', 'Із', 'запровадженням', 'біометричних', 'паспортів', 'активізувалися', 'шахраї', 'та', 'фірми-посередники', ',', 'які', 'пропонують', '«', 'прискорити', '»', 'оформлення', 'біометричного', 'паспорта', 'або', 'просто', 'оформити', 'цей', 'документ', '–', 'повідомляють', 'Першій', 'електронній', 'в', 'прес-службі', 'УДМС', 'України', 'в', 'Кіровоградській', 'області', '.', '</S>', '<S>', 'Розцінки', 'на', 'послуги', 'таких', 'посередників', 'починаються', 'від', '1000', 'грн', '.', '</S>', '<S>', 'Закінчується', '«', 'біометрична', 'афера', '»', 'в', 'кращому', 'випадку', 'звичайним', 'оформленням', 'документа', 'у', 'міграційній', 'службі', 'у', 'встановлений', 'законодавством', 'строк', ',', 'у', 'гіршому', '–', 'втратою', 'коштів']\n",
      "['--', '--', '--', '--', '--', '--', '--', '--', '--', '--', '--', '--', '--', '--', '--', '--', '--', '--', '--', '--', '--', '--', '--', '--', '--', '--', '--', '--', '--', '--', '--', '--', '--', '--', '--', '--', '--', '--', '--', '--', '--', '--', '--', '--', '--', '--', '--', '--', '--', '--', '--', '--', 'B-ОРГ', 'I-ОРГ', '--', '--', 'B-ОРГ', 'I-ОРГ', 'I-ОРГ', 'I-ОРГ', 'I-ОРГ', '--', '--', '--', '--', '--', '--', '--', '--', '--', '--', '--', '--', '--', '--', '--', '--', '--', '--', '--', '--', '--', '--', '--', '--', '--', '--', '--', 'B-ОРГ', 'I-ОРГ', '--', '--', '--', '--', '--', '--', '--', '--', '--', '--']\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction import DictVectorizer\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "from pymorphy2 import MorphAnalyzer\n",
    "import string\n",
    "\n",
    "# Rules\n",
    "\n",
    "def first_in_sentence(i, tokens):\n",
    "    if tokens[i-1] == '<S>':\n",
    "        return True\n",
    "    else: return False\n",
    "\n",
    "def is_title_case(i, tokens):\n",
    "    return tokens[i].title() == tokens[i]\n",
    "\n",
    "def is_upper_case(i, tokens):\n",
    "    return tokens[i].upper() == tokens[i]\n",
    "\n",
    "def is_punct(i, tokens):\n",
    "    return tokens[i] in string.punctuation\n",
    "\n",
    "def is_digit(i, tokens):\n",
    "    digits = '0123456789,.'\n",
    "    \n",
    "    for c in tokens[i]:\n",
    "        if c not in digits:\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "def lemma(i, tokens, morph):\n",
    "    try:\n",
    "        return str(morph.parse(tokens[i])[0].normal_form.lower())\n",
    "    except IndexError:\n",
    "        return '<NA>'\n",
    "    \n",
    "def pos(i, tokens, morph):\n",
    "    try:\n",
    "        return str(morph.parse(tokens[i])[0].tag.POS)\n",
    "    except IndexError:\n",
    "        return '<NA>'\n",
    "\n",
    "\n",
    "\n",
    "def my_little_baseline(tokens):\n",
    "    \n",
    "    labels = []\n",
    "    \n",
    "    for i in range(0, len(tokens)):\n",
    "        if is_title_case(i, tokens):\n",
    "            labels.append('B-ПЕРС')\n",
    "        else:\n",
    "            labels.append('--')\n",
    "    \n",
    "    return labels\n",
    "\n",
    "def feature_extractor(i, tokens):\n",
    "    \"\"\":rtype {feature: value, ...}\"\"\"\n",
    "    \n",
    "    global morph\n",
    "    \n",
    "    features = {}\n",
    "    \n",
    "    features['first_in_sentence'] = first_in_sentence(i, tokens)\n",
    "    features['is_title_case'] = is_title_case(i, tokens)\n",
    "    features['is_upper_case'] = is_upper_case(i, tokens)  \n",
    "    features['is_punct'] = is_punct(i, tokens)\n",
    "    features['is_digit'] = is_digit(i, tokens)\n",
    "    \n",
    "    features['lemma-1'] = lemma(i-1, tokens, morph)\n",
    "    features['lemma'] = lemma(i, tokens, morph)\n",
    "    features['lemma+1'] = lemma(i+1, tokens, morph)\n",
    "    \n",
    "    features['pos-1'] = pos(i-1, tokens, morph)\n",
    "    features['pos'] = pos(i, tokens, morph)\n",
    "    features['pos+1'] = pos(i+1, tokens, morph)\n",
    "\n",
    "    # lemma-1, lemma-2. lemma+1, lemma+2\n",
    "    # pos-1, pos-2, pos+1, pos+2\n",
    "    \n",
    "    # print(features)\n",
    "    \n",
    "    return features\n",
    "\n",
    "\n",
    "def vectorize(tokens, fit):\n",
    "    \n",
    "    feature_list = []\n",
    "    global vectorizer\n",
    "    \n",
    "    for i in range(0, len(tokens)):\n",
    "        feature_list.append(feature_extractor(i, tokens))\n",
    "    \n",
    "    if fit:\n",
    "        vec = vectorizer.fit_transform(feature_list) #.toarray()\n",
    "    else:\n",
    "        vec = vectorizer.transform(feature_list) #.toarray()\n",
    "    # print(\"\\nTotal number of features: \", len(vec.get_feature_names()))\n",
    "    \n",
    "    return vec\n",
    "\n",
    "morph = MorphAnalyzer(lang='uk')\n",
    "vectorizer = DictVectorizer()\n",
    "\n",
    "X_train = vectorize(train_tokens, fit=True)\n",
    "X_test = vectorize(test_tokens, fit=False)\n",
    "Y_train = train_labels\n",
    "Y_test = test_labels\n",
    "\n",
    "# pred_labels = my_little_baseline(test_tokens)\n",
    "# print(classification_report(test_labels, pred_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DTree Classification:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          --       0.93      0.97      0.95     69817\n",
      "       B-ЛОК       0.51      0.47      0.49       414\n",
      "       B-ОРГ       0.35      0.27      0.30       230\n",
      "      B-ПЕРС       0.58      0.75      0.65      1190\n",
      "      B-РІЗН       0.36      0.44      0.40       178\n",
      "       I-ЛОК       0.13      0.03      0.05      1071\n",
      "       I-ОРГ       0.55      0.05      0.09      1958\n",
      "      I-ПЕРС       0.15      0.08      0.10      2808\n",
      "      I-РІЗН       0.07      0.06      0.07       377\n",
      "\n",
      "   micro avg       0.89      0.89      0.89     78043\n",
      "   macro avg       0.40      0.35      0.34     78043\n",
      "weighted avg       0.86      0.89      0.87     78043\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ash/.pyenv/versions/3.6.5/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:758: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogReg Classification:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          --       0.92      1.00      0.96     69817\n",
      "       B-ЛОК       0.59      0.62      0.61       414\n",
      "       B-ОРГ       0.34      0.08      0.13       230\n",
      "      B-ПЕРС       0.66      0.76      0.71      1190\n",
      "      B-РІЗН       0.35      0.43      0.39       178\n",
      "       I-ЛОК       0.72      0.02      0.04      1071\n",
      "       I-ОРГ       0.76      0.04      0.07      1958\n",
      "      I-ПЕРС       0.55      0.05      0.09      2808\n",
      "      I-РІЗН       0.35      0.02      0.04       377\n",
      "\n",
      "   micro avg       0.91      0.91      0.91     78043\n",
      "   macro avg       0.58      0.34      0.34     78043\n",
      "weighted avg       0.89      0.91      0.88     78043\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ash/.pyenv/versions/3.6.5/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:166: FutureWarning: max_iter and tol parameters have been added in Perceptron in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perceptron Classification:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          --       0.92      0.97      0.94     69817\n",
      "       B-ЛОК       0.56      0.62      0.59       414\n",
      "       B-ОРГ       0.38      0.29      0.33       230\n",
      "      B-ПЕРС       0.77      0.49      0.60      1190\n",
      "      B-РІЗН       0.52      0.08      0.14       178\n",
      "       I-ЛОК       0.14      0.02      0.04      1071\n",
      "       I-ОРГ       0.83      0.02      0.04      1958\n",
      "      I-ПЕРС       0.13      0.06      0.08      2808\n",
      "      I-РІЗН       0.05      0.15      0.08       377\n",
      "\n",
      "   micro avg       0.88      0.88      0.88     78043\n",
      "   macro avg       0.48      0.30      0.32     78043\n",
      "weighted avg       0.86      0.88      0.86     78043\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Perceptron(alpha=0.0001, class_weight=None, early_stopping=False, eta0=1.0,\n",
       "      fit_intercept=True, max_iter=None, n_iter=None, n_iter_no_change=5,\n",
       "      n_jobs=None, penalty=None, random_state=0, shuffle=True, tol=None,\n",
       "      validation_fraction=0.1, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import tree\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression, Perceptron\n",
    "from sklearn import svm\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "def dtree_classify(X_train, X_test, Y_train, Y_test):\n",
    "    \"\"\"Run classification with Decision trees\"\"\"\n",
    "    \n",
    "    clf = tree.DecisionTreeClassifier()\n",
    "    clf = clf.fit(X_train, Y_train)\n",
    "    Y_pred = clf.predict(X_test)\n",
    "    \n",
    "    print('DTree Classification:')\n",
    "    print(classification_report(Y_test, Y_pred))\n",
    "    \n",
    "    return clf\n",
    "    \n",
    "def nbayes_classify(X_train, X_test, Y_train, Y_test):\n",
    "    clf = GaussianNB()\n",
    "    clf = clf.fit(X_train, Y_train)\n",
    "    Y_pred = clf.predict(X_test)\n",
    "    \n",
    "    print('Gaussian Naive Bayes Classification:')\n",
    "    print(classification_report(Y_test, Y_pred))\n",
    "\n",
    "    return clf\n",
    "    \n",
    "def knn_classify(X_train, X_test, Y_train, Y_test):\n",
    "    clf = KNeighborsClassifier(n_neighbors=3)\n",
    "    clf = clf.fit(X_train, Y_train)\n",
    "    Y_pred = clf.predict(X_test)\n",
    "    \n",
    "    print('kNN Classification:')\n",
    "    print(classification_report(Y_test, Y_pred))\n",
    "\n",
    "    return clf\n",
    "\n",
    "def logreg_classify(X_train, X_test, Y_train, Y_test):\n",
    "    clf = LogisticRegression(random_state=0, solver='lbfgs',\n",
    "                             multi_class='multinomial')\n",
    "    clf = clf.fit(X_train, Y_train)\n",
    "    Y_pred = clf.predict(X_test)\n",
    "    \n",
    "    print('LogReg Classification:')\n",
    "    print(classification_report(Y_test, Y_pred))\n",
    "\n",
    "    return clf\n",
    "\n",
    "def svm_classify(X_train, X_test, Y_train, Y_test):\n",
    "    clf = svm.SVC(gamma='scale')\n",
    "    clf = clf.fit(X_train, Y_train)\n",
    "    Y_pred = clf.predict(X_test)\n",
    "    \n",
    "    print('SVM Classification:')\n",
    "    print(classification_report(Y_test, Y_pred))\n",
    "\n",
    "    return clf\n",
    "\n",
    "def perceptron_classify(X_train, X_test, Y_train, Y_test):\n",
    "    clf = Perceptron()\n",
    "    clf = clf.fit(X_train, Y_train)\n",
    "    Y_pred = clf.predict(X_test)\n",
    "    \n",
    "    print('Perceptron Classification:')\n",
    "    print(classification_report(Y_test, Y_pred))\n",
    "\n",
    "    return clf\n",
    "\n",
    "dtree_classify(X_train, X_test, Y_train, Y_test)\n",
    "# nbayes_classify(X_train, X_test, Y_train, Y_test)\n",
    "# knn_classify(X_train, X_test, Y_train, Y_test)\n",
    "logreg_classify(X_train, X_test, Y_train, Y_test)\n",
    "# svm_classify(X_train, X_test, Y_train, Y_test)\n",
    "perceptron_classify(X_train, X_test, Y_train, Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s1 = ['1000']\n",
    "s2 = ['1.00']\n",
    "s3 = ['1,000q']\n",
    "\n",
    "is_digit(0, s3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
