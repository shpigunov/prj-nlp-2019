{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ukrainian Dependency Parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "actions = ['shift', 'right-arc', 'left-arc', 'reduce']\n",
    "elemnt = ('parent', 'child')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "У <-- домі\n",
      "домі <-- була\n",
      "римського <-- патриція\n",
      "патриція <-- домі\n",
      "Руфіна <-- патриція\n",
      "була <-- root\n",
      "прегарна <-- фреска\n",
      "фреска <-- була\n",
      ", <-- зображення\n",
      "зображення <-- фреска\n",
      "Венери <-- зображення\n",
      "та <-- Адоніса\n",
      "Адоніса <-- Венери\n",
      ". <-- була\n",
      "Якось <-- зібралися\n",
      "зібралися <-- root\n",
      "у <-- нього\n",
      "нього <-- зібралися\n",
      ", <-- ховаючися\n",
      "ховаючися <-- зібралися\n",
      "від <-- переслідувань\n",
      "переслідувань <-- ховаючися\n",
      ", <-- ховаючися\n",
      "одновірці <-- зібралися\n",
      "дружини <-- одновірці\n",
      "– <-- християнки\n",
      "християнки <-- дружини\n",
      ". <-- зібралися\n",
      "Й <-- узялися\n",
      "одразу <-- узялися\n",
      "ж <-- одразу\n",
      "узялися <-- root\n",
      "замазувати <-- узялися\n",
      "стіну <-- замазувати\n",
      ", <-- певні\n",
      "певні <-- узялися\n",
      "свого <-- права\n",
      "права <-- певні\n",
      "негайно <-- знищити\n",
      "знищити <-- права\n",
      "гріховне <-- мальовидло\n",
      ", <-- погляд\n",
      "як <-- погляд\n",
      "на <-- погляд\n",
      "їх <-- погляд\n",
      "погляд <-- гріховне\n",
      ", <-- погляд\n",
      "мальовидло <-- знищити\n",
      ". <-- узялися\n"
     ]
    }
   ],
   "source": [
    "from collections import OrderedDict\n",
    "from conllu import parse\n",
    "from enum import Enum\n",
    "\n",
    "\n",
    "def get_data(path):\n",
    "    with open(path, \"r\") as f:\n",
    "        data = f.read()\n",
    "\n",
    "    trees = parse(data)\n",
    "    return trees\n",
    "\n",
    "# debug mode\n",
    "trees = get_data('./corpus/uk_iu-ud-train.conllu')\n",
    "\n",
    "for i, tree in enumerate(trees):\n",
    "    for node in tree:\n",
    "        head = node[\"head\"]\n",
    "        try:\n",
    "            print(\"{} <-- {}\".format(node[\"form\"],\n",
    "                                     tree[head - 1][\"form\"]\n",
    "                                     if head > 0 else \"root\"))\n",
    "        except TypeError:\n",
    "            pass\n",
    "    \n",
    "    if i > 1:\n",
    "        break\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('id', 2),\n",
       "             ('form', 'зібралися'),\n",
       "             ('lemma', 'зібратися'),\n",
       "             ('upostag', 'VERB'),\n",
       "             ('xpostag', 'Vmeis-p'),\n",
       "             ('feats',\n",
       "              OrderedDict([('Aspect', 'Perf'),\n",
       "                           ('Mood', 'Ind'),\n",
       "                           ('Number', 'Plur'),\n",
       "                           ('Tense', 'Past'),\n",
       "                           ('VerbForm', 'Fin')])),\n",
       "             ('head', 0),\n",
       "             ('deprel', 'root'),\n",
       "             ('deps', [('root', 0)]),\n",
       "             ('misc',\n",
       "              OrderedDict([('Id', '000j'),\n",
       "                           ('LTranslit', 'zibratyśа'),\n",
       "                           ('Translit', 'zibralyśа')]))])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trees[1][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict([('id', 3), ('form', 'у'), ('lemma', 'у'), ('upostag', 'ADP'), ('xpostag', 'Spsg'), ('feats', OrderedDict([('Case', 'Gen')])), ('head', 4), ('deprel', 'case'), ('deps', [('case', 4)]), ('misc', OrderedDict([('Id', '000k'), ('LTranslit', 'u'), ('Translit', 'u')]))])\n"
     ]
    }
   ],
   "source": [
    "print(trees[1][2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint as pp\n",
    "\n",
    "def shift(stack, queue):\n",
    "    stack.append(queue.pop(0))\n",
    "    return stack, queue\n",
    "\n",
    "def right_arc(stack, queue, dep_arcs):\n",
    "    dep_arcs.append((stack[-1]['id'], queue[0]['id']))\n",
    "    stack, queue = shift(stack, queue)\n",
    "    return stack, queue, dep_arcs\n",
    "\n",
    "def left_arc(stack, queue, dep_arcs):\n",
    "    dep_arcs.append((queue[0]['id'], stack[-1]['id']))\n",
    "    stack.pop(-1)\n",
    "    return stack, queue, dep_arcs\n",
    "\n",
    "def reduce(stack):\n",
    "    stack.pop()\n",
    "    return stack\n",
    "\n",
    "def oracle_det(stack, queue, dep_arcs):\n",
    "    \"\"\"Deterministic oracle for trainiang. Requires a fully annotated tree.\"\"\"\n",
    "    global ROOT\n",
    "    \n",
    "    if stack[-1] and not queue[0]:\n",
    "        return 'reduce'\n",
    "    elif stack[-1]['head'] == queue[0]['id']:\n",
    "        return 'left_arc'\n",
    "    elif queue[0]['head'] == stack[-1]['id']:\n",
    "        return 'right_arc'\n",
    "    elif stack[-1][\"id\"] in [i[0] for i in dep_arcs] and \\\n",
    "         (queue[0][\"head\"] < stack[-1][\"id\"] or \\\n",
    "         [s for s in stack if s[\"head\"] == queue[0][\"id\"]]):\n",
    "        return 'reduce'    \n",
    "    else:\n",
    "        return 'shift'\n",
    "\n",
    "def feature_extract(stack, queue, dep_arcs):\n",
    "    features = {}\n",
    "    \n",
    "    # stk_0: form, lemma, postag, feats\n",
    "    features['stk_0_form'] = stack[-1]['form']\n",
    "    features['stk_0_lemma'] = stack[-1]['lemma']\n",
    "    features['stk_0_postag'] = stack[-1]['upostag']\n",
    "    \n",
    "    if stack[-1]['feats'] != None:\n",
    "        for feat in stack[-1]['feats'].keys():\n",
    "            features['stk_0_'+feat] = stack[-1]['feats'][feat]\n",
    "    \n",
    "    # queue_0: form, lemma, postag, feats\n",
    "    features['que_0_form'] = queue[0]['form']\n",
    "    features['que_0_lemma'] = queue[0]['lemma']\n",
    "    features['que_0_postag'] = queue[0]['upostag']\n",
    "    \n",
    "    if queue[0]['feats'] != None:\n",
    "        for feat in queue[0]['feats'].keys():\n",
    "            features['que_0_'+feat] = queue[0]['feats'][feat]\n",
    "    \n",
    "    # queue_1: form, postag\n",
    "    try:\n",
    "        features['que_1_form'] = queue[1]['form']\n",
    "        features['que_1_postag'] = queue[1]['upostag']\n",
    "    except IndexError:\n",
    "        pass\n",
    "\n",
    "    # queue_2: postag\n",
    "    try:\n",
    "        features['que_2_postag'] = queue[2]['upostag']\n",
    "    except IndexError:\n",
    "        pass\n",
    "    \n",
    "    # queue_3: postag\n",
    "    try:\n",
    "        features['que_3_postag'] = queue[3]['upostag']\n",
    "    except IndexError:\n",
    "        pass\n",
    "    \n",
    "    return features\n",
    "      \n",
    "    \n",
    "    \n",
    "ROOT = OrderedDict([('id', 0), ('form', 'ROOT'), ('lemma', 'ROOT'), ('upostag', 'ROOT'),\n",
    "                    ('xpostag', None), ('feats', None), ('head', None), ('deprel', None),\n",
    "                    ('deps', None), ('misc', None)])\n",
    "\n",
    "def dep_parse(tree, ret='xy', orcl='det'):\n",
    "    \"\"\"Parse dependencies for one sentence (tree)\"\"\"\n",
    "    \n",
    "    global ROOT\n",
    "    stack = [ROOT]\n",
    "    queue = tree[:]\n",
    "    dep_arcs = []\n",
    "    \n",
    "    x, y = [], []\n",
    "    \n",
    "    while len(stack) > 0 and len(queue) > 0:\n",
    "        \n",
    "        features = feature_extract(stack, queue, dep_arcs)        \n",
    "        \n",
    "        try:\n",
    "            if orcl == 'det':\n",
    "                action = oracle_det(stack, queue, dep_arcs)\n",
    "            elif orcl == 'ml':\n",
    "                action = oracle_ml(features)\n",
    "        except TypeError:\n",
    "            print(stack)\n",
    "            print(queue)\n",
    "            break\n",
    "        \n",
    "        x.append(features)\n",
    "        y.append(action)\n",
    "        \n",
    "        if action == 'reduce':\n",
    "            stack = reduce(stack)\n",
    "        elif action == 'left_arc':\n",
    "            stack, queue, dep_arcs = left_arc(stack, queue, dep_arcs)\n",
    "        elif action == 'right_arc':\n",
    "            stack, queue, dep_arcs = right_arc(stack, queue, dep_arcs)\n",
    "        elif action == 'shift':\n",
    "            stack, queue = shift(stack, queue)\n",
    "\n",
    "    if ret == 'xy':\n",
    "        return x, y    \n",
    "    elif ret == 'arcs':\n",
    "        return dep_arcs\n",
    "\n",
    "\n",
    "def filter_trees(trees): \n",
    "    \"\"\"Delete nodes from a tree where id is not an integer\"\"\"\n",
    "    return [[token for token in tree if type(token['id']) == int] for tree in trees]\n",
    "\n",
    "\n",
    "def prepare_data(path):\n",
    "\n",
    "    X, Y = [], []\n",
    "    trees = filter_trees(get_data(path))\n",
    "    \n",
    "    for tree in trees:\n",
    "        x, y = dep_parse(tree)\n",
    "        X.extend(x)\n",
    "        Y.extend(y)\n",
    "\n",
    "    assert len(X) == len(Y)\n",
    "    \n",
    "    return X, Y\n",
    "    \n",
    "train_path = \"./corpus/uk_iu-ud-train.conllu\"\n",
    "test_path = \"./corpus/uk_iu-ud-test.conllu\"\n",
    "\n",
    "X_train, Y_train = prepare_data(train_path)\n",
    "X_test, Y_test = prepare_data(test_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Vectorizing...\n"
     ]
    }
   ],
   "source": [
    "# Vectorize features\n",
    "\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "\n",
    "def vectorize(X_train, X_test):\n",
    "    \n",
    "    print('\\nVectorizing...')\n",
    "    v = DictVectorizer(sparse=True)\n",
    "    \n",
    "    vectorizer = v.fit(X_train)\n",
    "    v_train = vectorizer.transform(X_train)\n",
    "    v_test = vectorizer.transform(X_test)\n",
    "    \n",
    "    return v_train, v_test, vectorizer\n",
    "\n",
    "X_train, X_test, vectorizer = vectorize(X_train, X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    left_arc       0.85      0.93      0.89      7346\n",
      "      reduce       0.62      0.42      0.50      2552\n",
      "   right_arc       0.80      0.82      0.81      5935\n",
      "       shift       0.86      0.86      0.86     10336\n",
      "\n",
      "   micro avg       0.83      0.83      0.83     26169\n",
      "   macro avg       0.78      0.76      0.76     26169\n",
      "weighted avg       0.82      0.83      0.82     26169\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Try a different classifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "rfc = RandomForestClassifier(n_estimators=20, criterion='entropy', max_depth=None, n_jobs=-1, verbose=False)\n",
    "rfc.fit(X_train, Y_train)\n",
    "\n",
    "predicted = rfc.predict(X_test)\n",
    "print(classification_report(Y_test, predicted))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trying our Parser on New Text\n",
    "\n",
    "* First of all, we need to transform our text into a `conll`-like format.\n",
    "* To do that, we shall tokenize it with `tokenize_uk`;\n",
    "* Then, for each token, we will extract the necessary features using `pymorphy`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "\n",
    "from tokenize_uk import tokenize_words, tokenize_sents\n",
    "from pymorphy2 import MorphAnalyzer\n",
    "\n",
    "from pprint import pprint as pp\n",
    "\n",
    "# Just take a random paragraph from Kaydasheva Simya\n",
    "text = \"\"\"Яр в'ється гадюкою мiж крутими горами, мiж зеленими терасами; од яру на всi боки розбiглись, неначе гiлки дерева, глибокi рукави й поховались десь далеко в густих лiсах.\"\"\"\n",
    "\n",
    "def build_conll_trees(text):\n",
    "    \"\"\"\n",
    "    :param text: str - any input text in Ukrainian.\n",
    "    :rtype trees: list of dicts: {id, form, lemma, upostag, xpostag, feats, head, deprel, deps, misc}\n",
    "    \"\"\"\n",
    "\n",
    "    DET = ['інакший', 'його', 'тамтой', 'чий', 'їх', 'інш.', 'деякий', 'ввесь', 'ваш', \n",
    "           'ніякий', 'весь', 'інший', 'чийсь', 'жадний', 'другий', 'кожний', \n",
    "           'такий', 'оцей', 'скілька', 'цей', 'жодний', 'все', 'кілька', 'увесь', \n",
    "           'кожній', 'те', 'сей', 'ін.', 'отакий', 'котрий', 'усякий', 'самий', \n",
    "           'наш', 'усілякий', 'будь-який', 'сам', 'свій', 'всілякий', 'всенький', 'її', \n",
    "           'всякий', 'отой', 'небагато', 'який', 'їхній', 'той', 'якийсь', 'ин.', 'котрийсь', \n",
    "           'твій', 'мій', 'це']\n",
    "\n",
    "    PREP = [\"до\", \"на\"]\n",
    "\n",
    "    mapping = {\"ADJF\": \"ADJ\", \"ADJS\": \"ADJ\", \"COMP\": \"ADJ\", \"PRTF\": \"ADJ\",\n",
    "               \"PRTS\": \"ADJ\", \"GRND\": \"VERB\", \"NUMR\": \"NUM\", \"ADVB\": \"ADV\",\n",
    "               \"NPRO\": \"PRON\", \"PRED\": \"ADV\", \"PREP\": \"ADP\", \"PRCL\": \"PART\"}\n",
    "\n",
    "    def normalize_pos(word):\n",
    "        if word.tag.POS == \"CONJ\":\n",
    "            if \"coord\" in word.tag:\n",
    "                return \"CCONJ\"\n",
    "            else:\n",
    "                return \"SCONJ\"\n",
    "        elif \"PNCT\" in word.tag:\n",
    "            return \"PUNCT\"\n",
    "        elif word.normal_form in PREP:\n",
    "            return \"PREP\"\n",
    "        else:\n",
    "            return mapping.get(word.tag.POS, word.tag.POS)\n",
    "    \n",
    "    morph = MorphAnalyzer(lang='uk')\n",
    "    trees = []\n",
    "    \n",
    "    # Tokenize text into sentences\n",
    "    sentences = tokenize_sents(text)\n",
    "    for i, sentence in enumerate(sentences):\n",
    "        tree = []\n",
    "        \n",
    "        # Tokenize sentences into words\n",
    "        words = tokenize_words(sentence)\n",
    "        for j, word in enumerate(words):\n",
    "            \n",
    "            # We assume the pymorphy does a decent job and \n",
    "            # take the top word from the rank\n",
    "            token = morph.parse(word)[0]\n",
    "            \n",
    "            # Construct tree node:\n",
    "            \n",
    "            # Populate features dict\n",
    "            feat_names = [\n",
    "                'animacy', 'aspect', 'case', 'gender', 'involvement', 'mood', 'number',\n",
    "                'person', 'tense', 'transitivity', 'voice'\n",
    "            ]\n",
    "            feats = OrderedDict({})\n",
    "            for feat_name in feat_names:\n",
    "                exec(f\"\"\"if token.tag.{feat_name}: feats['{feat_name.title()}'] = str(token.tag.{feat_name}).title()\"\"\")\n",
    "\n",
    "            node = OrderedDict({\n",
    "                'id': j,\n",
    "                'form': word,\n",
    "                'lemma': token.normal_form,\n",
    "                'upostag': normalize_pos(token),\n",
    "                'xpostag': '',\n",
    "                'feats': feats,\n",
    "                'head': '',\n",
    "                'deprel': '',\n",
    "                'deps': '',\n",
    "                'misc': ''\n",
    "            })\n",
    "            \n",
    "            tree.append(node)\n",
    "        \n",
    "        trees.append(tree)\n",
    "    \n",
    "    return trees\n",
    "            \n",
    "def oracle_ml(features):\n",
    "    \"\"\"A machine learning classifier for untagged sentences.\n",
    "    :param features: dict - output of feature_extract(stack, queue, deps)\n",
    "    :rtype action: str - an action for the dependency parser\"\"\"\n",
    "    \n",
    "    global vectorizer\n",
    "    global rfc\n",
    "    \n",
    "    X = vectorizer.transform(features)\n",
    "    Y = rfc.predict(X)\n",
    "    \n",
    "    return Y[0]\n",
    "    \n",
    "    \n",
    "def parse_pipeline(text):\n",
    "    \"\"\"\n",
    "    :param text: str - any input text in Ukrainian.\n",
    "    :rtype dep_arcs: list of tuples: (parent_id, child_id) - list of arcs produced by the parser.\n",
    "    \"\"\"\n",
    "    trees = build_conll_trees(text)\n",
    "    arcs = []\n",
    "    \n",
    "    for tree in trees:\n",
    "        arcs.extend(dep_parse(tree, ret='arcs', orcl='ml'))\n",
    "        \n",
    "    return arcs\n",
    "    \n",
    "arcs = parse_pipeline(text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write results into a separate file\n",
    "\n",
    "import json \n",
    "\n",
    "with open('output.json', 'w+') as f:\n",
    "    f.write(json.dumps(arcs))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
